# configs/pipeline_config.yaml
data:
  train_path: "data/raw/train.txt"
  val_path: "data/raw/valid.txt"
  cleaning:
    remove_html: true
    remove_special_chars: true
    lowercase: true
    min_length: 20
    max_length: 100000
  dataloader:
    batch_size: 8
    max_length: 128
    num_workers: 2
    shuffle_buffer_size: 1000

tokenizer:
  min_frequency: 2
  special_tokens:
    <pad>: "[PAD]"
    <unk>: "[UNK]"
    <bos>: "[BOS]"
    <eos>: "[EOS]"

model:
  vocab_size: 32768
  hidden_size: 384
  num_hidden_layers: 6
  num_attention_heads: 6
  intermediate_size: 1536
  hidden_dropout_prob: 0.1
  attention_dropout_prob: 0.1
  max_position_embeddings: 512
  initializer_range: 0.02
  layer_norm_epsilon: "1e-12"
  tie_word_embeddings: true

training:
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 2000
  max_steps: 10000
  grad_clip_val: 1.0
  gradient_accumulation_steps: 4
  precision: 32
  accelerator: "mps"
  val_check_interval: 100
  gradient_checkpointing: true

logging:
  wandb_project: "transformer-lm"
  log_every_n_steps: 100
  save_every_n_steps: 1000
  monitor_metric: "val_loss"
  monitor_mode: "min"
  save_top_k: 2
